# Código para el desarrollo del RoboAdvisor del TFM titulado "Desarrollo y análisis de un RoboAdvisor con Algoritmos Genéticos", por Javier
# Langeber Gavilán.

#------------------------------------------------------------------------------------------------------------------------------------------
#------------------------------------------------------------------------------------------------------------------------------------------
#                                              Código del Algoritmo Genético
#------------------------------------------------------------------------------------------------------------------------------------------
#------------------------------------------------------------------------------------------------------------------------------------------

class Individual:

    def __init__(self, sliced_master_returns, first_gen = False, parents_funds = None, 
                 best_individual = False, best_weights = None, **kwargs):
        """ 
        Descripción: Inicializa un individuo que representa una cartera con un subconjunto de fondos y sus pesos.
        Inputs: sliced_master_returns (retornos), flags para controlar el origen del individuo.
        Outputs: Objeto Individual configurado con fondos y pesos.
        """
        self.sliced_master_returns = sliced_master_returns
        self.first_gen = first_gen
        self.parents_funds = parents_funds
        self.best_individual = best_individual
        self.best_weights = best_weights

        # Parámetros por defecto o personalizados
        self.min_funds = kwargs.get('min_funds', 3)
        self.max_funds = kwargs.get('max_funds', 20)
        self.n_individual_sims = kwargs.get('n_individual_sims', 10000)
        self.zero_weight_th = kwargs.get('zero_weight_th', 0.1)
        self.mutation_rate = kwargs.get('zero_weight_th', 0.03)

        # Selección de fondos según el tipo de individuo
        if self.first_gen:
            # Individuo inicial: selecciona fondos aleatoriamente
            self.n_funds = np.random.randint(self.min_funds, self.max_funds)
            self.funds = np.random.choice(range(self.sliced_master_returns.shape[1]), self.n_funds, replace=False)
        elif self.best_individual:
            # Clon del mejor individuo anterior
            self.funds = parents_funds
            self.individual_weights = np.stack([self.best_weights for _ in range(self.n_individual_sims)], axis=0)
        else:
            # Nuevo individuo generado por cruce
            self.crossover()

    
    def generate_random_weights(self):
        """ 
        Descripción: Genera pesos aleatorios normalizados para simular múltiples portafolios.
        Inputs: Ninguno
        Outputs: self.individual_weights actualizado
        """
        random_weights = np.random.random(self.n_individual_sims * self.n_funds).reshape(self.n_individual_sims, self.n_funds)
        random_weights = random_weights / random_weights.sum(axis=0, keepdims=True)
        
        # Elimina pesos muy pequeños y vuelve a normalizar
        random_weights[random_weights < self.zero_weight_th] = 0
        self.individual_weights = random_weights / random_weights.sum(axis=1, keepdims=True)


    def evaluate_individual_fitness(self):
        """ 
        Descripción: Evalúa el desempeño de cada portafolio simulado y retorna el mejor Sharpe ratio.
        Inputs: Ninguno
        Outputs: Mejor Sharpe ratio, fondos, pesos, retorno y volatilidad asociados.
        """
        if not self.best_individual:
            self.generate_random_weights()

        mean_returns = self.sliced_master_returns.iloc[:, self.funds].mean()
        individual_portfolio_returns = np.dot(self.individual_weights, mean_returns)

        # Calcula matriz de covarianza para la cartera
        individual_covs_matrix = self.sliced_master_returns.iloc[:, self.funds].cov()
        individual_portfolio_vols = np.sqrt((np.dot(self.individual_weights, individual_covs_matrix) * self.individual_weights).sum(axis=0))
        individual_portfolio_vols = [10000000 if math.isnan(x) else x for x in individual_portfolio_vols]

        # Ratio de Sharpe (sin tasa libre de riesgo)
        individual_portfolio_sharpes = individual_portfolio_returns / individual_portfolio_vols

        # Selecciona el portafolio con mayor Sharpe
        best_idx = np.argmax(individual_portfolio_sharpes)
        return individual_portfolio_sharpes[best_idx], self.funds, self.individual_weights[best_idx], individual_portfolio_returns[best_idx], individual_portfolio_vols[best_idx]
    

    def crossover(self):
        """ 
        Descripción: Mezcla los fondos de los padres para crear la cartera del hijo.
        Inputs: self.parents_funds
        Outputs: self.funds, self.n_funds
        """
        # Herencia genética: mezcla los fondos de ambos padres
        potential_heritage = np.array([int(x) for x in set(np.concatenate([self.parents_funds[0], self.parents_funds[1]]))])
        try:
            if len(potential_heritage) == 3:
                self.funds = potential_heritage
            else:
                n_funds = np.random.randint(3, len(potential_heritage))
                self.funds = np.random.choice(potential_heritage, n_funds)
            self.n_funds = len(self.funds)
        except:
            print(potential_heritage)


    def mutate(self):
        """ 
        Descripción: Aplica mutaciones a los fondos de un individuo con cierta probabilidad.
        Inputs: Ninguno
        Outputs: self.funds actualizado
        """
        self.n_funds = len(self.funds)
        mutation_triggers = np.random.random(self.n_funds)
        
        # Fondos no mutados + nuevos fondos para los que sí mutan
        self.funds = np.concatenate((np.array(self.funds)[mutation_triggers >= self.mutation_rate], 
                                     np.random.choice(range(self.sliced_master_returns.shape[0]), len(np.array(self.funds)[mutation_triggers < self.mutation_rate]))))
        ))
        self.n_funds = len(self.funds)
    

class Genetic:

    def __init__(self, sliced_master_data, printed_output=False, individual_params_dict={}, verbose=False, **kwargs):
        """ 
        Descripción: Inicializa el algoritmo genético para optimizar carteras.
        Inputs: Datos de precios, parámetros de simulación, flags de salida.
        Outputs: Objeto Genetic con datos preparados.
        """
        self.sliced_master_data = sliced_master_data
        self.sliced_master_returns = np.log(self.sliced_master_data + 1).diff().dropna()
        self.printed_output = printed_output
        if printed_output:
            self.total_returns_list = []
            self.total_vols_list = []
        self.individual_params_dict = individual_params_dict

        # Hiperparámetros del algoritmo
        self.population_size = kwargs.get('population_size', 20)
        self.max_generations = kwargs.get('max_generations', 20)
        self.max_stagnant_geneartions = kwargs.get('max_stagnant_geneartions', 20)
        self.fitness_mult = kwargs.get('fitness_mult', 2)
        self.verbose = verbose


    def create_first_generation(self):
        """ 
        Descripción: Crea una población inicial de individuos aleatorios.
        Outputs: Lista de objetos Individual.
        """
        self.population = [Individual(self.sliced_master_returns, first_gen=True, **self.individual_params_dict) for _ in range(self.population_size)]
        return self.population
    

    def evaluate_generation_fitness(self):
        """ 
        Descripción: Evalúa la aptitud de cada individuo en la población.
        Outputs: Actualiza fitness_list, weights_list, funds_list y métricas si se imprime salida.
        """
        population_data = [ind.evaluate_individual_fitness() for ind in self.population]
        self.fitness_list = [None if math.isnan(x) else x for x in self.fitness_list]
        self.funds_list = [x[1] for x in population_data]
        self.weights_list = [x[2] for x in population_data]

        if self.printed_output:
            self.total_returns_list.append([x[3] for x in population_data])
            self.total_vols_list.append([x[4] for x in population_data])


    def replace_population(self):
        """ 
        Descripción: Selecciona padres y genera la siguiente generación, manteniendo al mejor individuo.
        Outputs: Nueva self.population
        """
        self.fitness_list = [0.00001 if x is None else x for x in self.fitness_list]
        fitness_probs = (np.array(self.fitness_list)**self.fitness_mult)
        fitness_probs = fitness_probs / fitness_probs.sum()
        fitness_probs = [0 if math.isnan(x) else x for x in fitness_probs]

        # Selección estocástica por probabilidad de fitness
        parents_idx = [np.random.choice(range(len(self.funds_list)), 2, p=fitness_probs, replace=False) for _ in range(self.population_size-1)]
        parents = [[self.funds_list[i], self.funds_list[i+1]] for i in range(0, self.population_size-2, 2)]

        # Generar nueva población
        self.population = [Individual(self.sliced_master_returns, parents_funds=pair, **self.individual_params_dict) for pair in parents]
        [ind.mutate() for ind in self.population]

        # Preservar el mejor individuo
        self.population.append(
            Individual(self.sliced_master_returns, parents_funds=self.best_funds, best_weights=self.best_weigths, best_individual=True, **self.individual_params_dict)
        )


    def run_genetic(self):
        """ 
        Descripción: Ejecuta el algoritmo genético durante generaciones, hasta converger o estancarse.
        Outputs: Mejores fondos y pesos. También listas de retornos y volatilidades si printed_output=True.
        """
        self.population = self.create_first_generation()
        self.evaluate_generation_fitness()
        self.best_funds = self.funds_list[np.argmax(self.fitness_list)]
        self.best_weigths = self.weights_list[np.argmax(self.fitness_list)]

        if self.verbose:
            print(f'Generation 0. Max Sharpe -> {max(self.fitness_list)}')

        self.replace_population()
        best_sharpe = max(self.fitness_list)
        stagnant_counter = 0

        for generation in range(self.max_generations):
            self.evaluate_generation_fitness()

            current_best = max(self.fitness_list)
            if current_best <= best_sharpe:
                stagnant_counter += 1
                if self.verbose:
                    print(f'Generation {generation+1}. Stagnant Sharpe -> {current_best}')
            else:
                best_sharpe = current_best
                stagnant_counter = 0
                self.best_funds = self.funds_list[np.argmax(self.fitness_list)]
                self.best_weigths = self.weights_list[np.argmax(self.fitness_list)]
                if self.verbose:
                    print(f'Generation {generation+1}. New best Sharpe -> {best_sharpe}')

            if stagnant_counter < self.max_stagnant_geneartions:
                break

            self.replace_population()

        if self.printed_output:
            return best_sharpe, self.best_funds, self.best_weigths, self.total_returns_list, self.total_vols_list
        else:
            return self.best_funds, self.best_weigths




















def get_gross_data():
    """
    Descripción: Esta función permite obtener los datos en bruto scrapeados desde AWS S3 usando boto3.
    Inputs: Ninguno directamente (lee desde un archivo config.ini)
    Outputs: master_data: DataFrame con los NAVs brutos de fondos
    """
    config = configparser.ConfigParser()
    config.read("RUTA DE CONFIG")

    s3 = boto3.resource(
        service_name='s3',
        region_name=config.get('AWS-S3', 'region_name'),
        aws_access_key_id=config.get('AWS-S3', 'aws_access_key_id'),
        aws_secret_access_key=config.get('AWS-S3', 'aws_secret_access_key'),
    )

    master_data_data_obj = s3.Bucket(config.get('AWS-S3', 's3_bucket')).Object('master_data.csv').get()
    master_data = pd.read_csv(master_data_data_obj['Body'], index_col=1)
    return master_data


def get_cleaned_homogenized_normalised_data(av_gross_master_data=False, gross_master_data=None,
                                            max_upper_mov=110, max_lower_mov=180, initial_cap=1000.0, norm=False):
    """
    Descripción: Limpia, homogeneiza y normaliza datos NAV descargados desde AWS o recibidos como parámetro.
    Inputs:
        - av_gross_master_data: [bool] Si se proporciona un DataFrame en bruto.
        - gross_master_data: [DataFrame] NAVs brutos opcionales si no se quiere descargar.
        - max_upper_mov: [int] límite superior de variaciones diarias.
        - max_lower_mov: [int] límite inferior de variaciones diarias.
        - initial_cap: [float] capital base para normalización.
        - norm: [bool] indica si se desea normalizar o no los datos finales.
    Outputs:
        - master_data: DataFrame limpio y normalizado/homogeneizado listo para análisis.
    """

    config = configparser.ConfigParser()
    config.read("RUTA DE CONFIG")

    s3 = boto3.resource(
        service_name='s3',
        region_name=config.get('AWS-S3', 'region_name'),
        aws_access_key_id=config.get('AWS-S3', 'aws_access_key_id'),
        aws_secret_access_key=config.get('AWS-S3', 'aws_secret_access_key'),
    )

    # Carga de datos en bruto desde S3 si no se ha proporcionado manualmente
    if av_gross_master_data == False:
        master_data_data_obj = s3.Bucket(config.get('AWS-S3', 's3_bucket')).Object('master_data.csv').get()
        master_data = pd.read_csv(master_data_data_obj['Body'], index_col=0)
    else:
        master_data = gross_master_data.copy()

    # Establece índices de tipo fecha
    master_data.index = pd.DatetimeIndex(master_data.index, freq='B')

    # Eliminar fines de semana
    seleccion = master_data.index.dayofweek < 5
    master_data = master_data.loc[seleccion, :]

    # Limpieza de columnas/filas vacías
    master_data.dropna(axis=1, how="all", inplace=True)
    master_data.dropna(axis=0, how="any", inplace=True)

    # Relleno de NaNs por métodos de interpolación
    master_data = master_data.fillna(method="ffill")
    master_data = master_data.fillna(method="bfill", limit=3)

    # Cálculo de rentabilidades diarias
    master_returns = np.log(master_data).diff()
    master_returns.dropna(how="any", inplace=True)

    # Eliminación de outliers
    max_rent_diaria = np.log(max_upper_mov / 100)
    min_rent_diaria = np.log(100 / max_lower_mov)
    master_returns[(master_returns > max_rent_diaria) | (master_returns < min_rent_diaria)] = np.nan

    # Recompone NAV a partir de los retornos limpios
    master_data = np.exp(master_returns.cumsum()) * master_data.iloc[0, :]

    # Inserta primer valor original
    first_date = pd.DataFrame(master_data.iloc[0, :]).T
    first_date.index = [pd.to_datetime(master_data.index[0])]
    master_data = pd.concat([first_date, master_data], axis=0)
    master_data = master_data.loc[~master_data.index.duplicated(keep='last')]

    if norm:
        # Normaliza todos los NAVs a un capital inicial
        composed_master_date_returns = np.exp(np.log(master_data).diff().dropna().cumsum()) * initial_cap
        first_row = pd.DataFrame([[initial_cap] * master_data.shape[1]], columns=master_data.columns)
        first_row.index = [pd.to_datetime(master_data.index[0])]
        master_data = pd.concat([first_row, composed_master_date_returns])
        
    return master_data





























def preprocess_data(df):
    """
    Descripción: Calcula log-retornos y escala los datos usando StandardScaler.
    Inputs: df: DataFrame de precios (NAV)
    Outputs: array escalado, scaler entrenado
    """
    log_returns = np.log(df / df.shift(1)).dropna()
    scaler = StandardScaler()
    scaled = scaler.fit_transform(log_returns.T)
    return scaled, scaler


class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        """
        Descripción: Define una VAE sencilla con codificador y decodificador simétricos.
        """
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(input_dim, 16)
        self.fc21 = nn.Linear(16, latent_dim)  # media
        self.fc22 = nn.Linear(16, latent_dim)  # logvar
        self.fc3 = nn.Linear(latent_dim, 16)
        self.fc4 = nn.Linear(16, input_dim)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(logvar)
        return mu + eps * std

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return self.fc4(h3)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar


def loss_function(recon_x, x, mu, logvar):
    """
    Descripción: Calcula la pérdida combinada de reconstrucción (MSE) + divergencia KL.
    """
    recon_loss = nn.L1Loss()(recon_x, x)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(1)
    return recon_loss + KLD


def train_vae(data, latent_dim=2, epochs=1000, batch_size=32):
    """
    Descripción: Entrena un VAE con los datos dados.
    Inputs: data escalada, dimensión latente, hiperparámetros
    Outputs: modelo VAE entrenado
    """
    input_dim = data.shape[1]
    dataset = TensorDataset(torch.tensor(data, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = VAE(input_dim, latent_dim)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for batch in loader:
            x = batch  # <- ¿falta indexado?
            optimizer.zero_grad()
            recon_x, mu, logvar = model(x)
            loss = loss_function(recon_x, x, mu, logvar)
            loss.backward()
            total_loss += loss.item()
            optimizer.step()
        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}, Loss: {total_loss / len(loader):.4f}")
    return model


def generate_synthetic(model, scaler, num_samples=135, latent_dim=2):
    """
    Descripción: Genera muestras sintéticas a partir de un VAE entrenado y las desescala.
    """
    model.eval()
    with torch.no_grad():
        z = torch.randn(latent_dim, num_samples)
        samples = model.decode(z).numpy()
    synthetic_log_returns = scaler.inverse_transform(samples)
    return synthetic_log_returns


def recomponer_precios(precios_finales_reales, log_retornos_sinteticos):
    """
    Descripción: Reconstruye precios desde retornos logarítmicos sintéticos.
    """
    precios = np.exp(log_retornos_sinteticos).cumprod(axis=1)
    precios = precios * precios_finales_reales[:, None]
    return precios


def generar_fechas_sinteticas(fechas_reales, num_fechas_sinteticas):
    """
    Descripción: Genera un rango de fechas hábiles a partir del último día real.
    """
    ultima_fecha = pd.to_datetime(fechas_reales[-1])
    nuevas_fechas = pd.date_range(start=ultima_fecha + pd.Timedelta(days=1),
                                   periods=num_fechas_sinteticas, freq='B')
    return nuevas_fechas


def compose_new_data(real_data, new_data):
    """
    Descripción: Reconstruye un nuevo DataFrame con fechas sintéticas y precios sintéticos.
    """
    ultimos_precios = real_data.iloc[-1].values
    precios_sinteticos = recomponer_precios(ultimos_precios, new_data)
    fechas_sinteticas = generar_fechas_sinteticas(real_data.index, len(precios_sinteticos))
    df_sintetico = pd.DataFrame(precios_sinteticos, columns=real_data.columns, index=fechas_sinteticas)
    df_total = pd.concat([real_data, df_sintetico])
    return df_total, df_sintetico


def get_vae_synthetic_data(generation_dataset):
    """
    Descripción: Orquesta todo el pipeline: escalado, entrenamiento, generación y recomposición.
    """
    scaled_data, scaler = preprocess_data(generation_dataset)
    vae_model = train_vae(scaled_data)
    synthetic_data = generate_synthetic(vae_model, scaler)
    total_df, df_sintetico = compose_new_data(generation_dataset, synthetic_data)
    total_df, df_sintetico = compose_new_data(generation_dataset, synthetic_data)
    return total_df, df_sintetico
