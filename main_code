class Individual:

    def __init__(self, sliced_master_returns, first_gen = False, parents_funds = None, 
                 best_individual = False, best_weights = None, **kwargs):
        self.sliced_master_returns = sliced_master_returns
        self.first_gen = first_gen
        self.parents_funds = parents_funds
        self.best_individual = best_individual
        self.best_weights = best_weights

        self.min_funds = kwargs.get('min_funds', 3)
        self.max_funds = kwargs.get('max_funds', 20)
        self.n_individual_sims = kwargs.get('n_individual_sims', 10000)
        self.zero_weight_th = kwargs.get('zero_weight_th', 0.1)
        self.mutation_rate = kwargs.get('zero_weight_th', 0.03)

        if self.first_gen:
            self.n_funds = np.random.randint(self.min_funds, self.max_funds)
            self.funds = np.random.choice(range(self.sliced_master_returns.shape[1]), self.n_funds, replace=False)
        elif self.best_individual:
            self.funds = parents_funds
            self.individual_weights = np.stack([self.best_weights for _ in range(self.n_individual_sims)], axis=0)
        else:
            self.crossover()

    
    def generate_random_weights(self):
        random_weights = np.random.random(self.n_individual_sims * self.n_funds).reshape(
            self.n_individual_sims, self.n_funds)
        random_weights = random_weights / random_weights.sum(axis=1, keepdims=True)
        random_weights[random_weights < self.zero_weight_th] = 0
        self.individual_weights = random_weights / random_weights.sum(axis=1, keepdims=True)
        return 


    def evaluate_individual_fitness(self):
        if not self.best_individual:
            self.generate_random_weights()
        mean_returns = self.sliced_master_returns.iloc[:,self.funds].mean()
        individual_portfolio_returns = np.dot(self.individual_weights, mean_returns)
        # individual_portfolio_returns = [0 if math.isnan(x) else x for x in individual_portfolio_returns]
        individual_covs_matrix = self.sliced_master_returns.iloc[:,self.funds].cov()
        individual_portfolio_vols = np.sqrt((np.dot(self.individual_weights, individual_covs_matrix) * self.individual_weights).sum(axis=1))
        individual_portfolio_vols = [10000000 if math.isnan(x) else x for x in individual_portfolio_vols]
        individual_portfolio_sharpes = individual_portfolio_returns / individual_portfolio_vols
        portfolio_return = individual_portfolio_returns[np.argmax(individual_portfolio_sharpes)]
        portfolio_vol = individual_portfolio_vols[np.argmax(individual_portfolio_sharpes)]
        return individual_portfolio_sharpes.max(), self.funds, self.individual_weights[np.argmax(individual_portfolio_sharpes)], portfolio_return, portfolio_vol
    

    def crossover(self):
        
        # print(f'Fondos padre -> {len(self.parents_funds[0])}    ////    {self.parents_funds[0]}')
        # print(f'Fondos madre -> {len(self.parents_funds[1])}    ////    {self.parents_funds[1]}')
        # father_heritage = np.random.choice(self.parents_funds[0], np.random.randint(2,len(self.parents_funds[0])))
        # mother_heritage = np.random.choice(self.parents_funds[1], np.random.randint(2,len(self.parents_funds[1])))
        # self.funds = list(set(np.concatenate((father_heritage, mother_heritage)).tolist()))
        # self.n_funds = len(self.funds)
        # if len(self.funds) > self.max_funds:
        #     self.n_funds = np.random.randint(self.min_funds, self.max_funds)
        #     self.funds = np.random.choice(self.funds, self.n_funds)
        # elif len(self.funds) < self.min_funds:
        #     self.funds = list(set(np.concatenate([self.funds, np.random.choice(self.parents_funds[0], np.random.randint(3,18))])))
        #     self.n_funds = len(self.funds)
        # print(f'Fondos hijo -> {len(self.funds)}')
        # print('---------------')

        potential_heritage = np.array([int(x) for x in set(np.concatenate([self.parents_funds[0], self.parents_funds[1]]))])
        try:
            if len(potential_heritage) == 3:
                self.funds = potential_heritage
                self.n_funds = len(self.funds)
            else:
                n_funds = np.random.randint(3,len(potential_heritage))
                self.funds = np.random.choice(potential_heritage, n_funds, replace = False)
        except:
            print(potential_heritage)
        # print(f'Fondos hijo -> {len(self.funds)}')
        # print('---------------')
   

    def mutate(self):
        self.n_funds = len(self.funds)
        mutation_triggers = np.random.random(self.n_funds)
        self.funds = np.concatenate(([np.array(self.funds)[mutation_triggers >= self.mutation_rate], 
                                     np.random.choice(range(self.sliced_master_returns.shape[1]), len(np.array(self.funds)[mutation_triggers < self.mutation_rate]), replace=False)]))
        self.n_funds = len(self.funds)
        return
    

class Genetic:
    def __init__(self, sliced_master_data, printed_output = False, individual_params_dict = {}, verbose = False, **kwargs):
        self.sliced_master_data = sliced_master_data
        self.sliced_master_returns = np.log(self.sliced_master_data).diff().dropna()
        self.printed_output = printed_output
        if printed_output:
            self.total_returns_list = []
            self.total_vols_list = []
        self.individual_params_dict = individual_params_dict
        self.population_size = kwargs.get('population_size', 20)
        self.max_generations = kwargs.get('max_generations', 20)
        self.max_stagnant_geneartions = kwargs.get('max_stagnant_geneartions', 20)
        self.fitness_mult = kwargs.get('fitness_mult', 2)
        self.verbose = verbose
    

    def create_first_generation(self):
        self.population = [Individual(self.sliced_master_returns, first_gen=True, **self.individual_params_dict) for _ in range(self.population_size)]
        return self.population
    

    def evaluate_generation_fitness(self):
        population_data = [_.evaluate_individual_fitness() for _ in self.population]
        self.fitness_list = [_[0] for _ in population_data]
        self.fitness_list = [0.00001  if math.isnan(x) else x for x in self.fitness_list]
        self.funds_list = [_[1] for _ in population_data]
        self.weights_list = [_[2] for _ in population_data]
        if self.printed_output:
            self.total_returns_list.append([_[3] for _ in population_data])
            self.total_vols_list.append([_[4] for _ in population_data])


    def replace_population(self):
        self.fitness_list = [0.00001 if x < 0 else x for x in self.fitness_list]
        self.fitness_list = [0.00001 if math.isnan(x) else x for x in self.fitness_list]
        fitness_probs = (np.array(self.fitness_list)**self.fitness_mult) / (np.array(self.fitness_list)**self.fitness_mult).sum()
        fitness_probs = [0 if math.isnan(x) else x for x in fitness_probs]
        parents = np.array([np.random.choice(range(len(self.funds_list)), 2, p=fitness_probs, replace=False) for _ in range(self.population_size-1)]).flatten()
        parents = [[self.funds_list[parents[_*2]], self.funds_list[parents[_*2+1]]] for _ in range(self.population_size-1)]
        self.population = [Individual(self.sliced_master_returns, parents_funds = parents[_], **self.individual_params_dict) for _ in range(self.population_size-1)]
        [_.mutate() for _ in self.population]
        self.population.append(Individual(self.sliced_master_returns, parents_funds = self.best_funds, best_weights=self.best_weigths, best_individual=True, **self.individual_params_dict))
        

    def run_genetic(self):
        self.population = self.create_first_generation()
        self.evaluate_generation_fitness()
        self.best_funds = self.funds_list[np.argmax(self.fitness_list)]
        self.best_weigths = self.weights_list[np.argmax(self.fitness_list)]
        if self.verbose:
          print(f'Generation 0. Max Sharpe -> {max(self.fitness_list)}')
        self.replace_population()
        best_sharpe = max(self.fitness_list)
        stagnant_sharpe_counter = 0
        for generation in range(self.max_generations):
            self.evaluate_generation_fitness()
            if max(self.fitness_list) <= best_sharpe:
                if self.verbose:
                  print(f'Generation {generation+1}. Stagnant Sharpe -> {max(self.fitness_list)}')
                stagnant_sharpe_counter += 1
            elif max(self.fitness_list) > best_sharpe:
                best_sharpe = max(self.fitness_list)
                stagnant_sharpe_counter = 0
                if self.verbose:
                  print(f'Generation {generation+1}. New best Sharpe -> {max(self.fitness_list)}')
                self.best_funds = self.funds_list[np.argmax(self.fitness_list)]
                self.best_weigths = self.weights_list[np.argmax(self.fitness_list)]
            if stagnant_sharpe_counter == self.max_stagnant_geneartions:
                break
            self.replace_population()
        if self.printed_output:
            return best_sharpe, self.best_funds, self.best_weigths, self.total_returns_list, self.total_vols_list
        else:
            self.best_funds, self.best_weigths




















def get_gross_data():
    """
    Esta función permite otener los datos en bruto scrapeados de las webs
    """
    config = configparser.ConfigParser()
    config.read("C:/Users/jlang/OneDrive/Escritorio/UPM_TFM/TFM_ETSII_JLG/src/config.ini")

    s3 = boto3.resource(
        service_name = 's3',
        region_name = config.get('AWS-S3', 'region_name'),
        aws_access_key_id = config.get('AWS-S3', 'aws_access_key_id'),
        aws_secret_access_key = config.get('AWS-S3', 'aws_secret_access_key'),
    )

    master_data_data_obj = s3.Bucket(config.get('AWS-S3',\
                                    's3_bucket')).Object('master_data.csv').get()
    master_data = pd.read_csv(master_data_data_obj['Body'], index_col = 0)
    return master_data


def get_cleaned_homogenized_normalised_data(av_gross_master_data = False, gross_master_data = None, max_upper_mov = 110, max_lower_mov =180, initial_cap=1000.0, norm = False):
    """
    Esta función obtiene los datos de los fondos limpios y homogeneizados, descargandolos
    en primer lugar de AWS. Limpia los valores con NaN de los fondos, y homogeiniza sus 
    NAVs para evitar saltos de valor por error en las bases de datos, además de ponerlos 
    todos con inicio en el mismo precio.

    INPUTs
        - [None]
    OUTPUTs
        - master_data: [DF] con los NAVs de todos los fondos limpios, homogeneizados y normalizados
    """

    # OBTENCION DE DATOS

    config = configparser.ConfigParser()
    config.read("C:/Users/jlang/OneDrive/Escritorio/UPM_TFM/TFM_ETSII_JLG/src/config.ini")

    s3 = boto3.resource(
        service_name = 's3',
        region_name = config.get('AWS-S3', 'region_name'),
        aws_access_key_id = config.get('AWS-S3', 'aws_access_key_id'),
        aws_secret_access_key = config.get('AWS-S3', 'aws_secret_access_key'),
    )

    if av_gross_master_data == False:
        master_data_data_obj = s3.Bucket(config.get('AWS-S3',\
                                        's3_bucket')).Object('master_data.csv').get()
        master_data = pd.read_csv(master_data_data_obj['Body'], index_col = 0)
    else:
        master_data = gross_master_data
    
    # LIMPIEZA DE DATOS

    master_data.index = pd.DatetimeIndex(master_data.index)

    # Elimino los fondos que no tienen la cantidad de datos suficiente
    # fondos_no_elim = (master_data.iloc[-365*yearfrom:].isnull().sum(axis=0)/master_data.iloc[-365*yearfrom:].shape[0]) < nan_threshold
    # master_data = master_data.loc[:, fondos_no_elim]
    # No elimino ningún fondo utilizando como filtro la cantidad de NaNs. Ese filtro
    # No tiene en cuenta la existencia o no de los fondos en determinados intervalos de tiempo
    # Lo que hago es, al hacer el backtesting, miro la ventana rolada de 6M anterior,
    # y solo utilizo los fondos cuyo precio del primer día de la ventana es diferente al 
    # último precio de la ventana (doy por hecho que esos son los fondos que 
    # estaban vivos en la ventan anterior, y por tanto, los que puedo utilizar)

    # elimino los fines de semana
    seleccion = master_data.index.dayofweek < 5
    master_data = master_data.loc[seleccion, :]

    # elimino las filas y columnas que no tienen ningun dato
    master_data.dropna(axis=1, how="all", inplace=True)
    master_data.dropna(axis=0, how="all", inplace=True)

    # relleno los NaN para poder usar para cálculos
    master_data = master_data.fillna(method="ffill")
    master_data = master_data.fillna(method="bfill")

    # HOMOGENEIZACIÓN DE DATOS

    master_returns = np.log(master_data).diff().dropna()
    max_rent_diaria = np.log(max_upper_mov/100)
    min_rent_diaria = np.log(100/max_lower_mov)
    master_returns[(master_returns > max_rent_diaria) | (master_returns < min_rent_diaria)] = 0
    master_data = np.exp(master_returns.cumsum())*master_data.iloc[0,:]
    first_date = pd.DataFrame(master_data.iloc[0,:]).T
    first_date.index = [pd.to_datetime(master_data.index[0])]
    master_data = pd.concat([first_date, master_data], axis=0)
    master_data = master_data[~master_data.index.duplicated(keep='first')]

    if norm:
        composed_master_date_returns = np.exp(np.log(master_data).diff().dropna().cumsum())*initial_cap
        first_date = pd.DataFrame([[1000] * len(master_data.columns)], columns=master_data.columns)
        first_date.index = [pd.to_datetime(master_data.index[0])]
        master_data = pd.concat([first_date, composed_master_date_returns])
        
    return master_data





























def preprocess_data(df):
    log_returns = np.log(df / df.shift(1)).dropna()
    scaler = StandardScaler()
    scaled = scaler.fit_transform(log_returns)
    return scaled, scaler


class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(input_dim, 16)
        self.fc21 = nn.Linear(16, latent_dim)  # mean
        self.fc22 = nn.Linear(16, latent_dim)  # logvar
        self.fc3 = nn.Linear(latent_dim, 16)
        self.fc4 = nn.Linear(16, input_dim)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return self.fc4(h3)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    recon_loss = nn.MSELoss()(recon_x, x)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
    return recon_loss + KLD


def train_vae(data, latent_dim=2, epochs=1000, batch_size=32):
    input_dim = data.shape[1]
    dataset = TensorDataset(torch.tensor(data, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = VAE(input_dim, latent_dim)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for batch in loader:
            x = batch[0]
            optimizer.zero_grad()
            recon_x, mu, logvar = model(x)
            loss = loss_function(recon_x, x, mu, logvar)
            loss.backward()
            total_loss += loss.item()
            optimizer.step()
        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}, Loss: {total_loss / len(loader):.4f}")
    return model


def generate_synthetic(model, scaler, num_samples=135, latent_dim=2):
    model.eval()
    with torch.no_grad():
        z = torch.randn(num_samples, latent_dim)
        samples = model.decode(z).numpy()
    # Desescalar
    synthetic_log_returns = scaler.inverse_transform(samples)
    return synthetic_log_returns

def recomponer_precios(precios_finales_reales, log_retornos_sinteticos):
    precios = np.exp(log_retornos_sinteticos).cumprod(axis=0)
    precios = precios * precios_finales_reales
    return precios

def generar_fechas_sinteticas(fechas_reales, num_fechas_sinteticas):
    ultima_fecha = pd.to_datetime(fechas_reales[-1])
    nuevas_fechas = pd.date_range(start=ultima_fecha + pd.Timedelta(days=1),
                                   periods=num_fechas_sinteticas, freq='B')  # B = días hábiles
    return nuevas_fechas


def compose_new_data(real_data, new_data):
    ultimos_precios = real_data.iloc[-1].values
    precios_sinteticos = recomponer_precios(ultimos_precios, new_data)
    fechas_sinteticas = generar_fechas_sinteticas(real_data.index, len(precios_sinteticos))
    df_sintetico = pd.DataFrame(precios_sinteticos, columns=real_data.columns, index=fechas_sinteticas)
    df_total = pd.concat([real_data, df_sintetico])
    return df_total, df_sintetico




def get_vae_synthetic_data(generation_dataset):
    scaled_data, scaler = preprocess_data(generation_dataset)
    vae_model = train_vae(scaled_data)
    synthetic_data = generate_synthetic(vae_model, scaler)
    total_df, df_sintetico = compose_new_data(generation_dataset, synthetic_data)
    total_df, df_sintetico = compose_new_data(generation_dataset, synthetic_data)
    return total_df, df_sintetico
